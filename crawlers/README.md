# Crawler Challenge
Simple crawler to get thrending threads of Reddit and send then to a Telegram Bot.

## Techonologies 

This solution is written in Python 3.6.5 using the following technologies:
- [beautiful soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - Python libary to pulling data from HTML and XML.
- [requests](http://docs.python-requests.org/en/master/) - Python libary to consuption of HTTP.
- [telegram bot](https://core.telegram.org/bots) - Bot API HTTP based interface, used to set up and create bot for telegram.


## Requirements
To run this solution you need a bot TOKEN generated by Telegram.

To create a TOKEN and create a bot to consume the application, you must to open telegram app, and
send a message to [botfather](https://telegram.me/botfather) asking for a token with the /token command.

After that, make a copy of *.env.example* file and save that as *.env*, save your token inside *.env* file.

**WARNING: NEVER COMMIT YOUR BOT TOKEN.**

## Running Solution
The two ways that you can run this solution is locally or using docker, for both cases follow the given steps:

### Locally
In order to run this solution script you must have Python 3.6, PIP and this project dependencies installed. To install Python and PIP, please refeer to their official documentation.

To install all dependencies, simple run:
> $ pip install -r requirements.txt

### With Docker
Also this solution was built using docker. You will only need docker installed (please refeer to [official docker docs](https://docs.docker.com/install/)). If you want to use docker in order to prevent install anything in your machine, you can use the Dockerfile and docker-compose.yml of this project.

This solution image contains only the minimal necessary to solve this challenge. Because of that I choose to use Python3.6 image with Linux Alpine 3.7.

To need this image, go to root of this project:
> $ cd ../

Build the image, this will already download all dependencies listed on requirements.txt
> $ docker-compose build

Run the container
> $ docker-compose up

To access the container bash, you must list all containers running and get your container id:
> $ docker ps

Then connect to this terminal:
> $ docker exec -it 5324 ash

## Usage
You can extract data of Reddit passing the subrredits that you want to take the number of upvotes, comments link, the name of the subreddit, posts url and their titles.  

To do that, navigate to project folder:
> $ cd crawlers/api_telegram

Then call *run.py* passing subreddits desired as parameters:
> $ python run.py 

After that you can search for @YourBotName on telegram, and the bot will be started and pass to you the commands to use it.